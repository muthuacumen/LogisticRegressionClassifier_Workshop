{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-title",
   "metadata": {},
   "source": [
    "# Will This Person Repay Their Bank Loan?\n",
    "## A Beginner's Guide to Logistic Regression Classification\n",
    "\n",
    "---\n",
    "\n",
    "### The Story Behind This Notebook\n",
    "\n",
    "Imagine you work at a bank. Every day, people walk in and ask to borrow money. Your job is to decide:\n",
    "\n",
    "> **\"Should we give this person a loan ‚Äî or are they too risky?\"**\n",
    "\n",
    "In the old days, a loan officer would sit down with the applicant, look at their salary, their credit history, and their debts ‚Äî and then make a gut-feeling decision. Today, banks use **machine learning** to make this decision automatically, faster, and more consistently.\n",
    "\n",
    "In this notebook, we will build a model that looks at a customer's **annual income** and predicts whether they are likely to **repay the loan (‚úÖ)** or **default on it (‚ùå)**.\n",
    "\n",
    "The technique we use is called **Logistic Regression** ‚Äî and by the end, you'll understand exactly how it works.\n",
    "\n",
    "---\n",
    "\n",
    "### What You Will Learn\n",
    "\n",
    "| Step | Concept | Plain English Summary |\n",
    "| :--- | :--- | :--- |\n",
    "| 0 | Linear Regression Recap | Why the tool we already know **doesn't work** for yes/no decisions |\n",
    "| 1 | The Sigmoid Function | The clever S-shaped curve that converts any number into a **probability** |\n",
    "| 2 | Statistical Classification | What it means to sort people into **categories** |\n",
    "| 3 | Building the Classifier | Actually **training a model** in Python with scikit-learn |\n",
    "| 4 | Log-Loss | How we measure **how wrong** our model is (so we can make it better) |\n",
    "| 5 | Talking Points | 3 key ideas to **explain this to others** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-setup-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup: Importing Our Tools\n",
    "\n",
    "Before we start, we need to load the Python libraries we'll use throughout this notebook.\n",
    "\n",
    "Think of these as the **toolbox** we open before starting a project:\n",
    "- `numpy` ‚Üí handles numbers and math\n",
    "- `matplotlib` ‚Üí draws our charts\n",
    "- `sklearn` ‚Üí the machine learning library that does the heavy lifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"All libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-s0-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 0: Why Linear Regression Won't Work Here (Recap)\n",
    "\n",
    "### What is Linear Regression?\n",
    "\n",
    "You've already seen **Linear Regression** ‚Äî it's a tool that draws a straight line through data to predict a **continuous number**. For example: *\"If a customer earns \\$50,000 a year, how much of a loan can they afford?\"* ‚Äî that's a continuous answer (e.g., \\$12,500).\n",
    "\n",
    "### But what if the answer is just YES or NO?\n",
    "\n",
    "Our loan repayment problem is different. We don't want a number ‚Äî we want a **decision**:\n",
    "- **1** = Yes, this person will repay the loan ‚úÖ\n",
    "- **0** = No, this person will default ‚ùå\n",
    "\n",
    "Let's try to use Linear Regression anyway and see what happens..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-s0-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our dataset: Annual income (in $1,000s) vs. whether the person repaid their loan\n",
    "# Income values range from $15,000 to $75,000\n",
    "annual_income = np.array([\n",
    "    15, 18, 22, 25, 28, 30, 32, 35, 38, 40,\n",
    "    43, 45, 48, 50, 53, 55, 58, 62, 68, 75\n",
    "]).reshape(-1, 1)\n",
    "\n",
    "# 0 = Defaulted (did NOT repay), 1 = Repaid successfully\n",
    "loan_repaid = np.array([\n",
    "    0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
    "    1, 0, 1, 1, 1, 1, 1, 1, 1, 1\n",
    "])\n",
    "\n",
    "# --- Fit a Linear Regression model ---\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(annual_income, loan_repaid)\n",
    "linear_predictions = linear_model.predict(annual_income)\n",
    "\n",
    "# --- Plot the result ---\n",
    "plt.figure(figsize=(11, 6))\n",
    "plt.scatter(annual_income, loan_repaid, color='red', zorder=20, s=80,\n",
    "            label='Actual Data (0=Defaulted, 1=Repaid)')\n",
    "plt.plot(annual_income, linear_predictions, color='blue', linewidth=2.5,\n",
    "         label='Linear Regression Line')\n",
    "\n",
    "# Draw horizontal lines to mark the only valid outcomes (0 and 1)\n",
    "plt.axhline(y=0, color='black', linestyle='--', alpha=0.5, label='Minimum valid output (0)')\n",
    "plt.axhline(y=1, color='black', linestyle='-.', alpha=0.5, label='Maximum valid output (1)')\n",
    "\n",
    "# Highlight the \"danger zone\" where the line goes below 0\n",
    "x_fill = np.linspace(10, 32, 100)\n",
    "y_fill = linear_model.predict(x_fill.reshape(-1, 1))\n",
    "plt.fill_between(x_fill, y_fill, 0, where=(y_fill < 0), alpha=0.2, color='orange',\n",
    "                 label='Problem: Predictions below 0!')\n",
    "\n",
    "plt.xlabel('Annual Income (in $1,000s)')\n",
    "plt.ylabel('Predicted Outcome')\n",
    "plt.title('The Problem with Using Linear Regression for a Yes/No Decision')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True, linestyle='--', alpha=0.4)\n",
    "plt.show()\n",
    "\n",
    "# Show some problematic predictions\n",
    "print(\"Predictions from Linear Regression for low-income customers:\")\n",
    "for income in [10, 15, 20]:\n",
    "    pred = linear_model.predict([[income]])[0]\n",
    "    print(f\"  Income = ${income},000 ‚Üí Predicted outcome = {pred:.2f}  ‚Üê This makes no sense!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-s0-explain",
   "metadata": {},
   "source": [
    "### What Just Went Wrong? ü§î\n",
    "\n",
    "Look at the orange shaded area ‚Äî that's where Linear Regression predicts values **below 0** (like -0.3 or -0.5). But what does a \"probability of -0.3\" even mean? **Nothing.** Probabilities must always be between 0 and 1.\n",
    "\n",
    "This is the core problem:\n",
    "\n",
    "> **Linear Regression was designed for continuous outputs (like price or temperature). It was NOT designed for yes/no classification problems.**\n",
    "\n",
    "We need a smarter tool ‚Äî one that is **naturally constrained** to produce values between 0 and 1. That tool is **Logistic Regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-s1-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: The Secret Weapon ‚Äî The Sigmoid (S-Curve) Function\n",
    "\n",
    "### The Big Idea\n",
    "\n",
    "Logistic Regression's secret is a clever mathematical function called the **Sigmoid function** (also called the **logistic function**). It takes ANY number you throw at it ‚Äî positive, negative, huge, tiny ‚Äî and squishes it into a value between **0 and 1**.\n",
    "\n",
    "This makes it perfect for representing **probability**.\n",
    "\n",
    "### The Formula\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Where:\n",
    "- $z$ is just a regular number (could be anything: -100, 0, 5, 1000...)\n",
    "- $e$ is Euler's number (~2.718), the base of the natural logarithm\n",
    "- $\\sigma(z)$ is the output ‚Äî always a value between 0 and 1\n",
    "\n",
    "In our bank loan case:\n",
    "- $z$ is calculated from the customer's income (using a formula the model learns)\n",
    "- $\\sigma(z)$ = **\"the probability this customer will repay the loan\"**\n",
    "\n",
    "### Why is it Called a \"Sigmoid\"?\n",
    "\n",
    "Because it looks like the letter **S**! As you'll see in the chart below:\n",
    "- Very low incomes ‚Üí probability close to **0** (almost certainly will default)\n",
    "- Very high incomes ‚Üí probability close to **1** (almost certainly will repay)\n",
    "- Medium incomes ‚Üí probability near **0.5** (it's a coin flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-s1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function: squishes any number into the range (0, 1)\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Generate a range of z values to plot the S-curve\n",
    "z_values = np.linspace(-8, 8, 200)\n",
    "sigmoid_values = sigmoid(z_values)\n",
    "\n",
    "# --- Plot ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# LEFT PLOT: Pure sigmoid curve\n",
    "ax1 = axes[0]\n",
    "ax1.plot(z_values, sigmoid_values, color='purple', linewidth=3, label='Sigmoid Curve')\n",
    "ax1.axhline(y=0.5, color='green', linestyle='--', linewidth=1.5, label='Decision threshold (0.5)')\n",
    "ax1.axhline(y=0, color='grey', linestyle=':', alpha=0.6)\n",
    "ax1.axhline(y=1, color='grey', linestyle=':', alpha=0.6)\n",
    "ax1.fill_between(z_values, sigmoid_values, 0.5, where=(sigmoid_values >= 0.5),\n",
    "                 alpha=0.15, color='green', label='Likely to Repay (‚â• 0.5)')\n",
    "ax1.fill_between(z_values, sigmoid_values, 0.5, where=(sigmoid_values < 0.5),\n",
    "                 alpha=0.15, color='red', label='Likely to Default (< 0.5)')\n",
    "ax1.set_xlabel('z (any real number)', fontsize=12)\n",
    "ax1.set_ylabel('œÉ(z) ‚Äî Probability of Repaying', fontsize=12)\n",
    "ax1.set_title('The Sigmoid Function ‚Äî Always Between 0 and 1', fontsize=13)\n",
    "ax1.legend()\n",
    "ax1.grid(True, linestyle='--', alpha=0.4)\n",
    "\n",
    "# RIGHT PLOT: Annotated with intuitive examples\n",
    "ax2 = axes[1]\n",
    "ax2.plot(z_values, sigmoid_values, color='purple', linewidth=3)\n",
    "ax2.axhline(y=0.5, color='green', linestyle='--', linewidth=1.5)\n",
    "\n",
    "# Annotate interesting points on the curve\n",
    "annotations = [\n",
    "    (-6, sigmoid(-6), 'Very low income\\n‚Üí Almost certain default\\n(~0.25% chance of repaying)'),\n",
    "    (0,  sigmoid(0),  'Average income\\n‚Üí 50/50 chance\\n(could go either way)'),\n",
    "    (6,  sigmoid(6),  'Very high income\\n‚Üí Almost certain repayment\\n(~99.75% chance)'),\n",
    "]\n",
    "\n",
    "for x, y, label in annotations:\n",
    "    ax2.annotate(label, xy=(x, y), xytext=(x - 0.5, y + 0.18),\n",
    "                 fontsize=9, ha='center',\n",
    "                 bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow', alpha=0.8),\n",
    "                 arrowprops=dict(arrowstyle='->', color='black'))\n",
    "    ax2.scatter([x], [y], color='red', zorder=5, s=80)\n",
    "\n",
    "ax2.set_xlabel('z (calculated from customer income)', fontsize=12)\n",
    "ax2.set_ylabel('Probability of Repaying the Loan', fontsize=12)\n",
    "ax2.set_title('What the Sigmoid Tells Us About Loan Applicants', fontsize=13)\n",
    "ax2.grid(True, linestyle='--', alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show how the sigmoid behaves at specific values\n",
    "print(\"Sigmoid function output for different z values:\")\n",
    "for z in [-10, -5, -2, 0, 2, 5, 10]:\n",
    "    print(f\"  œÉ({z:4d}) = {sigmoid(z):.6f}  ‚Üí Always between 0 and 1 ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-s1-explain",
   "metadata": {},
   "source": [
    "### Key Takeaway üí°\n",
    "\n",
    "No matter what number you put into the sigmoid function, the output is **always between 0 and 1**. This is exactly what we need to represent a probability.\n",
    "\n",
    "The sigmoid is like a **translator**: it takes a raw score (which could be any number) and converts it into a clean probability that everyone can understand:\n",
    "\n",
    "| Sigmoid Output | What It Means for Our Bank |\n",
    "| :--- | :--- |\n",
    "| 0.95 | \"95% confident this person will repay ‚Äî APPROVE the loan\" |\n",
    "| 0.50 | \"It's a coin flip ‚Äî we might want more information\" |\n",
    "| 0.08 | \"Only 8% chance of repayment ‚Äî REJECT the loan\" |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-s2-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Understanding Statistical Classification\n",
    "\n",
    "### What Does \"Classification\" Mean?\n",
    "\n",
    "**Classification** is the process of looking at some information about something and assigning it to a **category** (also called a *class*).\n",
    "\n",
    "You do this every day without thinking:\n",
    "- You look at the sky ‚Üí \"cloudy\" or \"sunny\" ‚òÅÔ∏è‚òÄÔ∏è\n",
    "- You get an email ‚Üí \"spam\" or \"not spam\" üìß\n",
    "- A doctor reads an X-ray ‚Üí \"cancerous\" or \"benign\" üè•\n",
    "- The bank reviews an application ‚Üí **\"will repay\" or \"will default\"** üè¶\n",
    "\n",
    "### Key Vocabulary (in Plain English)\n",
    "\n",
    "| Term | Plain English | Our Loan Example |\n",
    "| :--- | :--- | :--- |\n",
    "| **Classifier** | The algorithm that makes the sorting decision | Logistic Regression |\n",
    "| **Features** | The pieces of information we use to decide | Annual income, credit score |\n",
    "| **Classes / Labels** | The possible categories we sort into | `0 = Default`, `1 = Repaid` |\n",
    "| **Decision Boundary** | The invisible line that separates the two classes | Income level where 50% chance flips |\n",
    "| **Training Data** | Historical examples the model learns from | Past customers and their repayment history |\n",
    "\n",
    "### Visualizing a Decision Boundary\n",
    "\n",
    "The chart below shows how the model learns to draw an invisible line between \"safe\" and \"risky\" loan applicants. Any new customer that falls on one side of the line is approved; on the other side, rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-s2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Simulate two groups of bank customers:\n",
    "#   Group 0 (defaulted): lower income, higher debt\n",
    "#   Group 1 (repaid):    higher income, lower debt\n",
    "# We use 2 features so we can visualize them on a 2D chart.\n",
    "np.random.seed(42)\n",
    "X_customers, y_customers = make_blobs(\n",
    "    n_samples=120,\n",
    "    centers=[(30, 70), (65, 25)],   # (income, debt-to-income ratio)\n",
    "    n_features=2,\n",
    "    cluster_std=8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Draw the decision boundary manually (a straight line)\n",
    "x_boundary = np.linspace(5, 95, 100)\n",
    "y_boundary = -1.1 * x_boundary + 110  # This is approximately where the boundary falls\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(11, 7))\n",
    "\n",
    "# Plot the two groups of customers\n",
    "defaulted = y_customers == 0\n",
    "repaid    = y_customers == 1\n",
    "plt.scatter(X_customers[defaulted, 0], X_customers[defaulted, 1],\n",
    "            color='#e74c3c', edgecolor='black', s=80, label='Defaulted (Class 0) ‚ùå')\n",
    "plt.scatter(X_customers[repaid, 0], X_customers[repaid, 1],\n",
    "            color='#2ecc71', edgecolor='black', s=80, label='Repaid (Class 1) ‚úÖ')\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.plot(x_boundary, y_boundary, color='navy', linestyle='--', linewidth=2.5,\n",
    "         label='Decision Boundary')\n",
    "\n",
    "# Shade the two regions\n",
    "plt.fill_between(x_boundary, y_boundary, 110,\n",
    "                 alpha=0.07, color='red', label='\"Reject\" Zone')\n",
    "plt.fill_between(x_boundary, 0, y_boundary,\n",
    "                 alpha=0.07, color='green', label='\"Approve\" Zone')\n",
    "\n",
    "# Add labels for a new applicant\n",
    "plt.scatter([55], [40], color='gold', edgecolor='black', s=200, zorder=10,\n",
    "            marker='*', label='New Applicant (Income=$55k, Debt=40%)')\n",
    "plt.annotate('New Applicant\\n‚Üí Falls in APPROVE zone',\n",
    "             xy=(55, 40), xytext=(68, 55),\n",
    "             fontsize=10, ha='center',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightyellow'),\n",
    "             arrowprops=dict(arrowstyle='->', color='black'))\n",
    "\n",
    "plt.xlabel('Annual Income ($1,000s)', fontsize=12)\n",
    "plt.ylabel('Debt-to-Income Ratio (%)', fontsize=12)\n",
    "plt.title('Classification: Separating Loan Defaulters from Repayers', fontsize=14)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlim(5, 95)\n",
    "plt.ylim(0, 105)\n",
    "plt.grid(True, linestyle='--', alpha=0.4)\n",
    "plt.show()\n",
    "\n",
    "print(\"The model's job: find the best possible decision boundary line\")\n",
    "print(\"that correctly separates the two classes in our training data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-s2-explain",
   "metadata": {},
   "source": [
    "### What the Chart Tells Us\n",
    "\n",
    "The dashed blue line is the **decision boundary** ‚Äî the bank's automatic dividing line between \"approve\" and \"reject\".\n",
    "\n",
    "- **Red dots** = past customers who defaulted (the bank lost money on them)\n",
    "- **Green dots** = past customers who repaid successfully\n",
    "- **Gold star** = a new applicant the bank needs to evaluate today\n",
    "\n",
    "The model learned this boundary by studying thousands of past customers. Now, when a new applicant comes in, the bank just checks which side of the line they fall on ‚Äî and makes an instant decision.\n",
    "\n",
    "> **Real-world note:** Banks actually use dozens (or even hundreds) of features ‚Äî not just 2. The decision boundary becomes a multi-dimensional surface, but the idea is exactly the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-s3-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Building Our Loan Repayment Classifier\n",
    "\n",
    "### From Theory to Code\n",
    "\n",
    "Now let's build the actual classifier! We'll use our original dataset (annual income vs. loan outcome) and train a Logistic Regression model using `scikit-learn`.\n",
    "\n",
    "### The Decision Rule (Threshold)\n",
    "\n",
    "Once the sigmoid gives us a probability, we apply a simple decision rule:\n",
    "\n",
    "| Probability of Repayment | Bank's Decision |\n",
    "| :--- | :--- |\n",
    "| ‚â• 0.5 (50%) | **Approve** the loan (classify as `1 = Repaid`) |\n",
    "| < 0.5 (50%) | **Reject** the loan (classify as `0 = Default`) |\n",
    "\n",
    "The 0.5 threshold is the default, but in real life banks might raise it to 0.7 to be more conservative (reject more people) or lower it to reduce false rejections. This is a **business decision**, not just a mathematical one.\n",
    "\n",
    "### Let's Build It Step by Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-s3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 1: Our Dataset\n",
    "# ============================================================\n",
    "# Annual income in $1,000s. E.g., 15 means $15,000/year\n",
    "annual_income = np.array([\n",
    "    15, 18, 22, 25, 28, 30, 32, 35, 38, 40,\n",
    "    43, 45, 48, 50, 53, 55, 58, 62, 68, 75\n",
    "]).reshape(-1, 1)\n",
    "\n",
    "# 0 = Defaulted, 1 = Repaid\n",
    "loan_repaid = np.array([\n",
    "    0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
    "    1, 0, 1, 1, 1, 1, 1, 1, 1, 1\n",
    "])\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2: Create and Train the Logistic Regression Model\n",
    "# ============================================================\n",
    "# Think of .fit() as \"learning from historical data\"\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(annual_income, loan_repaid)\n",
    "print(\"‚úÖ Model trained successfully!\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 3: Make Predictions\n",
    "# ============================================================\n",
    "# Get probabilities for all training customers\n",
    "# predict_proba returns [[prob_class_0, prob_class_1], ...]\n",
    "probabilities = log_reg.predict_proba(annual_income)\n",
    "\n",
    "# Get the final classification (0 or 1) for each customer\n",
    "predictions = log_reg.predict(annual_income)\n",
    "\n",
    "print(\"\\nüìä First 5 customers' predictions:\")\n",
    "print(f\"{'Income':>10} {'Actual':>10} {'P(Default)':>12} {'P(Repay)':>12} {'Decision':>12}\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(5):\n",
    "    inc    = annual_income[i][0]\n",
    "    actual = loan_repaid[i]\n",
    "    p0     = probabilities[i][0]\n",
    "    p1     = probabilities[i][1]\n",
    "    dec    = predictions[i]\n",
    "    flag   = '‚úÖ' if dec == actual else '‚ùå'\n",
    "    print(f\"  ${inc:>5}k     {actual:>7}   {p0:>10.1%}   {p1:>10.1%}   {dec:>5} {flag}\")\n",
    "\n",
    "print(\"\\n... (truncated for brevity)\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 4: Evaluate the Model\n",
    "# ============================================================\n",
    "accuracy = accuracy_score(loan_repaid, predictions)\n",
    "print(f\"\\nüéØ Model Accuracy: {accuracy * 100:.1f}%\")\n",
    "print(f\"   ‚Üí The model correctly classified {int(accuracy * len(loan_repaid))} out of {len(loan_repaid)} customers.\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 5: Predict for a Brand-New Applicant\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NEW LOAN APPLICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "new_applicant_income = 42  # $42,000 per year\n",
    "new_applicant = np.array([[new_applicant_income]])\n",
    "\n",
    "prob_default = log_reg.predict_proba(new_applicant)[0][0]\n",
    "prob_repay   = log_reg.predict_proba(new_applicant)[0][1]\n",
    "decision     = log_reg.predict(new_applicant)[0]\n",
    "\n",
    "print(f\"  Applicant's Annual Income: ${new_applicant_income},000\")\n",
    "print(f\"  Probability of Default:    {prob_default:.1%}\")\n",
    "print(f\"  Probability of Repayment:  {prob_repay:.1%}\")\n",
    "print(f\"  Model's Decision:          {'APPROVE ‚úÖ' if decision == 1 else 'REJECT ‚ùå'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-s3-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 6: Visualize the Trained Model\n",
    "# ============================================================\n",
    "\n",
    "# Generate a smooth range of income values for the S-curve\n",
    "x_smooth = np.linspace(5, 85, 300).reshape(-1, 1)\n",
    "y_prob_smooth = log_reg.predict_proba(x_smooth)[:, 1]  # probability of repayment\n",
    "\n",
    "# Find the decision boundary (where probability = 50%)\n",
    "# Solved from: 0 = intercept + coef * x  ‚Üí  x = -intercept / coef\n",
    "decision_boundary = -log_reg.intercept_[0] / log_reg.coef_[0][0]\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(13, 7))\n",
    "\n",
    "# Shade the approve/reject zones\n",
    "plt.axvspan(5, decision_boundary, alpha=0.07, color='red', label='Reject Zone')\n",
    "plt.axvspan(decision_boundary, 85, alpha=0.07, color='green', label='Approve Zone')\n",
    "\n",
    "# Plot the S-curve\n",
    "plt.plot(x_smooth, y_prob_smooth, color='#8e44ad', linewidth=3,\n",
    "         label='Logistic Regression S-Curve')\n",
    "\n",
    "# Plot the actual training data\n",
    "plt.scatter(annual_income[loan_repaid == 0], loan_repaid[loan_repaid == 0],\n",
    "            color='#e74c3c', s=100, zorder=10, label='Actual: Defaulted ‚ùå')\n",
    "plt.scatter(annual_income[loan_repaid == 1], loan_repaid[loan_repaid == 1],\n",
    "            color='#27ae60', s=100, zorder=10, label='Actual: Repaid ‚úÖ')\n",
    "\n",
    "# Mark the decision threshold line (p=0.5)\n",
    "plt.axhline(y=0.5, color='grey', linestyle='--', alpha=0.8, label='Decision Threshold (50%)')\n",
    "\n",
    "# Mark the decision boundary (income value where probability = 0.5)\n",
    "plt.axvline(x=decision_boundary, color='navy', linestyle=':', linewidth=2,\n",
    "            label=f'Decision Boundary at ${decision_boundary:.0f}k income')\n",
    "plt.annotate(f'Decision Boundary\\n${decision_boundary:.0f},000/year',\n",
    "             xy=(decision_boundary, 0.5), xytext=(decision_boundary + 10, 0.35),\n",
    "             fontsize=10, ha='center',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightyellow'),\n",
    "             arrowprops=dict(arrowstyle='->', color='black'))\n",
    "\n",
    "# Mark the new applicant\n",
    "plt.scatter([new_applicant_income], [prob_repay], color='gold', edgecolor='black',\n",
    "            s=250, zorder=15, marker='*', label=f'New Applicant (${new_applicant_income}k)')\n",
    "\n",
    "plt.xlabel('Annual Income (in $1,000s)', fontsize=13)\n",
    "plt.ylabel('Probability of Repaying the Loan', fontsize=13)\n",
    "plt.title('Logistic Regression: Predicting Loan Repayment Probability', fontsize=15)\n",
    "plt.legend(loc='center right', fontsize=9)\n",
    "plt.grid(True, linestyle='--', alpha=0.4)\n",
    "plt.ylim(-0.05, 1.1)\n",
    "plt.show()\n",
    "\n",
    "print(f\"The model's decision boundary: customers earning more than ~${decision_boundary:.0f},000/year\")\n",
    "print(f\"are classified as 'likely to repay' (probability ‚â• 50%).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-s3-explain",
   "metadata": {},
   "source": [
    "### Reading the Chart\n",
    "\n",
    "- The **purple S-curve** is what our model learned from the historical data. It represents the probability of repayment at each income level.\n",
    "- The **red zone** on the left = income too low ‚Üí model says \"REJECT\"\n",
    "- The **green zone** on the right = income high enough ‚Üí model says \"APPROVE\"\n",
    "- The **navy dotted line** is the exact income level where the model switches from reject to approve\n",
    "- The **gold star** is our new applicant ‚Äî notice where it falls on the curve!\n",
    "\n",
    "> **In real banking systems:** The model would use many more features (credit score, existing debts, employment length, etc.). But the core logic is identical ‚Äî the sigmoid converts everything into a probability, and the threshold makes the final call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-s4-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Log-Loss ‚Äî How Do We Know if Our Model is Good?\n",
    "\n",
    "### The Problem with Just Counting Correct Answers\n",
    "\n",
    "Imagine two bank employees evaluating the same loan application:\n",
    "- **Employee A** says: *\"I'm 99% sure this person will default\"* ‚Üí then they do default ‚úÖ\n",
    "- **Employee B** says: *\"I'm 51% sure this person will default\"* ‚Üí then they do default ‚úÖ\n",
    "\n",
    "Both got it right, but **Employee A was much more confident and accurate**. Simple accuracy (\"how many did you get right?\") treats them the same. That's not fair.\n",
    "\n",
    "This is where **Log-Loss** comes in. It rewards confident correct predictions and severely punishes confident wrong predictions.\n",
    "\n",
    "### The Formula\n",
    "\n",
    "For a single prediction:\n",
    "\n",
    "$$\\text{Loss} = -\\left[ y \\cdot \\log(p) + (1 - y) \\cdot \\log(1 - p) \\right]$$\n",
    "\n",
    "Breaking this down:\n",
    "- $y$ = the **true outcome** (1 if repaid, 0 if defaulted)\n",
    "- $p$ = the **model's predicted probability** that the person will repay\n",
    "- When $y=1$: loss = $-\\log(p)$ ‚Üí low loss if $p$ is high (model was confidently right)\n",
    "- When $y=0$: loss = $-\\log(1-p)$ ‚Üí low loss if $p$ is low (model correctly said \"unlikely to repay\")\n",
    "\n",
    "For the whole dataset of $N$ customers:\n",
    "\n",
    "$$\\text{Total Log-Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\cdot \\log(p_i) + (1 - y_i) \\cdot \\log(1 - p_i) \\right]$$\n",
    "\n",
    "### Plain English Summary\n",
    "\n",
    "| Scenario | Loss |\n",
    "| :--- | :--- |\n",
    "| Model says 95% chance of repayment ‚Üí Customer **REPAYS** | Very low (model was right and confident) |\n",
    "| Model says 50% chance of repayment ‚Üí Customer **REPAYS** | Medium (model was right but unsure) |\n",
    "| Model says 95% chance of repayment ‚Üí Customer **DEFAULTS** | Very high (model was catastrophically wrong) |\n",
    "\n",
    "> **Lower log-loss = better model.** The goal of training is to minimize this number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-s4-intuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Intuition: How Log-Loss Penalizes Confident Wrong Predictions\n",
    "# ============================================================\n",
    "\n",
    "# Case 1: True label y=1 (customer REPAID)\n",
    "# Case 2: True label y=0 (customer DEFAULTED)\n",
    "\n",
    "p_range = np.linspace(0.001, 0.999, 300)  # All possible predicted probabilities\n",
    "loss_y1 = -np.log(p_range)          # Loss when true label is 1\n",
    "loss_y0 = -np.log(1 - p_range)      # Loss when true label is 0\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# ---- LEFT: When Customer Actually REPAID (y=1) ----\n",
    "ax1 = axes[0]\n",
    "ax1.plot(p_range, loss_y1, color='#27ae60', linewidth=3)\n",
    "ax1.set_title('Customer Actually REPAID (y = 1)\\nHow loss varies with predicted probability', fontsize=12)\n",
    "ax1.set_xlabel('p  (Model\\'s predicted probability that customer REPAYS)', fontsize=10)\n",
    "ax1.set_ylabel('Log-Loss (Penalty)', fontsize=11)\n",
    "\n",
    "# Annotate key points\n",
    "ax1.annotate('p=0.95, loss‚âà0.05\\n(Confident AND correct ‚Üí tiny penalty)',\n",
    "             xy=(0.95, -np.log(0.95)), xytext=(0.7, 1.5),\n",
    "             fontsize=9, ha='center',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7),\n",
    "             arrowprops=dict(arrowstyle='->', color='black'))\n",
    "ax1.annotate('p=0.05, loss‚âà3.0\\n(Confident WRONG ‚Üí huge penalty!)',\n",
    "             xy=(0.05, -np.log(0.05)), xytext=(0.3, 3.8),\n",
    "             fontsize=9, ha='center',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightsalmon', alpha=0.7),\n",
    "             arrowprops=dict(arrowstyle='->', color='black'))\n",
    "ax1.scatter([0.95, 0.05], [-np.log(0.95), -np.log(0.05)],\n",
    "            color='red', zorder=5, s=80)\n",
    "ax1.set_ylim(-0.2, 5)\n",
    "ax1.grid(True, linestyle='--', alpha=0.4)\n",
    "\n",
    "# ---- RIGHT: When Customer Actually DEFAULTED (y=0) ----\n",
    "ax2 = axes[1]\n",
    "ax2.plot(p_range, loss_y0, color='#e74c3c', linewidth=3)\n",
    "ax2.set_title('Customer Actually DEFAULTED (y = 0)\\nHow loss varies with predicted probability', fontsize=12)\n",
    "ax2.set_xlabel('p  (Model\\'s predicted probability that customer REPAYS)', fontsize=10)\n",
    "ax2.set_ylabel('Log-Loss (Penalty)', fontsize=11)\n",
    "\n",
    "ax2.annotate('p=0.05, loss‚âà0.05\\n(Correctly predicted DEFAULT ‚Üí tiny penalty)',\n",
    "             xy=(0.05, -np.log(1-0.05)), xytext=(0.3, 1.5),\n",
    "             fontsize=9, ha='center',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7),\n",
    "             arrowprops=dict(arrowstyle='->', color='black'))\n",
    "ax2.annotate('p=0.95, loss‚âà3.0\\n(Said \"will repay\" but they DEFAULTED ‚Üí huge penalty!)',\n",
    "             xy=(0.95, -np.log(1-0.95)), xytext=(0.65, 3.8),\n",
    "             fontsize=9, ha='center',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightsalmon', alpha=0.7),\n",
    "             arrowprops=dict(arrowstyle='->', color='black'))\n",
    "ax2.scatter([0.05, 0.95], [-np.log(1-0.05), -np.log(1-0.95)],\n",
    "            color='navy', zorder=5, s=80)\n",
    "ax2.set_ylim(-0.2, 5)\n",
    "ax2.grid(True, linestyle='--', alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-s4-calculate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Calculate Log-Loss for Our Bank Loan Model\n",
    "# ============================================================\n",
    "\n",
    "def calculate_log_loss(y_true, y_prob):\n",
    "    \"\"\"\n",
    "    Compute the average log-loss (cross-entropy loss) for a set of predictions.\n",
    "    \n",
    "    Parameters:\n",
    "        y_true  : array of actual outcomes (0 or 1)\n",
    "        y_prob  : array of predicted probabilities for class 1 (repayment)\n",
    "    \n",
    "    Returns:\n",
    "        float: The average log-loss across all samples\n",
    "    \"\"\"\n",
    "    eps = 1e-15  # Tiny buffer to prevent log(0) errors\n",
    "    y_prob = np.clip(y_prob, eps, 1 - eps)\n",
    "    individual_losses = -(y_true * np.log(y_prob) + (1 - y_true) * np.log(1 - y_prob))\n",
    "    return individual_losses, np.mean(individual_losses)\n",
    "\n",
    "\n",
    "# Get probabilities of repayment from our trained model\n",
    "p_repay = probabilities[:, 1]\n",
    "\n",
    "# Calculate losses\n",
    "individual_losses, total_log_loss = calculate_log_loss(loan_repaid, p_repay)\n",
    "\n",
    "# --- Display a customer-by-customer breakdown ---\n",
    "print(\"Customer-by-Customer Log-Loss Breakdown:\")\n",
    "print(f\"{'Customer':>10} {'Income':>8} {'Actual':>8} {'P(Repay)':>10} {'Loss':>10} {'Note'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i in range(len(loan_repaid)):\n",
    "    inc    = annual_income[i][0]\n",
    "    actual = loan_repaid[i]\n",
    "    prob   = p_repay[i]\n",
    "    loss   = individual_losses[i]\n",
    "    # Flag: was this a bad prediction? (high loss = bad)\n",
    "    note = \"‚ö†Ô∏è  High penalty!\" if loss > 1.0 else \"‚úÖ Good prediction\"\n",
    "    print(f\"  #{i+1:>4}     ${inc:>4}k   {actual:>6}   {prob:>9.1%}   {loss:>9.4f}   {note}\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(f\"\\nüìä Total Log-Loss (averaged over all customers): {total_log_loss:.4f}\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"  A log-loss of {total_log_loss:.4f} means our model's probability estimates\")\n",
    "print(f\"  are well-calibrated ‚Äî it's confident when it should be, and uncertain\")\n",
    "print(f\"  when the data is ambiguous.\")\n",
    "print(f\"\\n  Perfect model ‚Üí Log-Loss = 0\")\n",
    "print(f\"  Random guessing ‚Üí Log-Loss ‚âà 0.693\")\n",
    "print(f\"  Our model ‚Üí Log-Loss = {total_log_loss:.4f}  (lower than random = better than guessing!) ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-s4-explain",
   "metadata": {},
   "source": [
    "### How Training Actually Works (The Big Picture)\n",
    "\n",
    "When we call `log_reg.fit(...)`, here's what happens under the hood:\n",
    "\n",
    "1. The model starts with **random guesses** for its internal parameters (weights)\n",
    "2. It makes predictions and **calculates the log-loss** ‚Äî how wrong it is\n",
    "3. It **adjusts the parameters** slightly to reduce the log-loss\n",
    "4. Steps 2‚Äì3 repeat **hundreds of times** until the loss stops decreasing\n",
    "\n",
    "This process is called **Gradient Descent** ‚Äî like rolling a ball downhill to find the lowest point (minimum loss).\n",
    "\n",
    "```\n",
    "Start ‚Üí High Loss\n",
    "        ‚Üì  (adjust parameters)\n",
    "        ‚Üì  (adjust parameters)\n",
    "        ‚Üì  (adjust parameters)\n",
    "End   ‚Üí Low Loss = Well-trained model!\n",
    "```\n",
    "\n",
    "Every classification model ‚Äî from simple logistic regression to complex neural networks ‚Äî uses some version of this same process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-summary-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: The Full Picture ‚Äî Everything We Covered\n",
    "\n",
    "Let's connect all the pieces together with one final visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-summary-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Final Summary: The Complete Logistic Regression Pipeline\n",
    "# ============================================================\n",
    "\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "fig.suptitle('Complete Loan Repayment Classifier ‚Äî Summary', fontsize=16, fontweight='bold')\n",
    "\n",
    "# ---- Plot 1: Raw Data ----\n",
    "ax1 = fig.add_subplot(1, 3, 1)\n",
    "ax1.scatter(annual_income[loan_repaid == 0], loan_repaid[loan_repaid == 0],\n",
    "            color='#e74c3c', s=80, label='Defaulted ‚ùå')\n",
    "ax1.scatter(annual_income[loan_repaid == 1], loan_repaid[loan_repaid == 1],\n",
    "            color='#27ae60', s=80, label='Repaid ‚úÖ')\n",
    "ax1.set_title('Step 1: Raw Training Data', fontsize=12)\n",
    "ax1.set_xlabel('Annual Income ($k)')\n",
    "ax1.set_ylabel('Loan Outcome')\n",
    "ax1.set_yticks([0, 1])\n",
    "ax1.set_yticklabels(['Defaulted (0)', 'Repaid (1)'])\n",
    "ax1.legend()\n",
    "ax1.grid(True, linestyle='--', alpha=0.4)\n",
    "\n",
    "# ---- Plot 2: The Sigmoid Fit ----\n",
    "ax2 = fig.add_subplot(1, 3, 2)\n",
    "x_smooth = np.linspace(5, 85, 300).reshape(-1, 1)\n",
    "y_prob_smooth = log_reg.predict_proba(x_smooth)[:, 1]\n",
    "ax2.plot(x_smooth, y_prob_smooth, color='#8e44ad', linewidth=3, label='Learned S-Curve')\n",
    "ax2.axhline(y=0.5, color='grey', linestyle='--', alpha=0.8, label='50% Threshold')\n",
    "ax2.axvline(x=decision_boundary, color='navy', linestyle=':', linewidth=2,\n",
    "            label=f'Boundary: ${decision_boundary:.0f}k')\n",
    "ax2.scatter(annual_income[loan_repaid == 0], loan_repaid[loan_repaid == 0],\n",
    "            color='#e74c3c', s=50, alpha=0.6)\n",
    "ax2.scatter(annual_income[loan_repaid == 1], loan_repaid[loan_repaid == 1],\n",
    "            color='#27ae60', s=50, alpha=0.6)\n",
    "ax2.set_title('Step 2: Model Learns the Sigmoid', fontsize=12)\n",
    "ax2.set_xlabel('Annual Income ($k)')\n",
    "ax2.set_ylabel('P(Loan Repaid)')\n",
    "ax2.legend(fontsize=8)\n",
    "ax2.grid(True, linestyle='--', alpha=0.4)\n",
    "\n",
    "# ---- Plot 3: Loss for Each Customer ----\n",
    "ax3 = fig.add_subplot(1, 3, 3)\n",
    "colors = ['#27ae60' if l < 0.5 else ('#f39c12' if l < 1.0 else '#e74c3c')\n",
    "          for l in individual_losses]\n",
    "bars = ax3.bar(range(1, len(individual_losses) + 1), individual_losses,\n",
    "               color=colors, edgecolor='black', linewidth=0.5)\n",
    "ax3.axhline(y=total_log_loss, color='navy', linestyle='--', linewidth=2,\n",
    "            label=f'Avg Log-Loss = {total_log_loss:.3f}')\n",
    "ax3.set_title('Step 3: Log-Loss Per Customer', fontsize=12)\n",
    "ax3.set_xlabel('Customer #')\n",
    "ax3.set_ylabel('Individual Log-Loss')\n",
    "ax3.legend()\n",
    "ax3.grid(True, linestyle='--', alpha=0.4, axis='y')\n",
    "\n",
    "# Add a color legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#27ae60', label='Low loss (confident & correct)'),\n",
    "    Patch(facecolor='#f39c12', label='Medium loss'),\n",
    "    Patch(facecolor='#e74c3c', label='High loss (confident & wrong)'),\n",
    "]\n",
    "ax3.legend(handles=legend_elements, fontsize=8, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Summary:\")\n",
    "print(f\"  Customers in dataset  : {len(loan_repaid)}\")\n",
    "print(f\"  Model accuracy        : {accuracy_score(loan_repaid, log_reg.predict(annual_income)) * 100:.1f}%\")\n",
    "print(f\"  Average log-loss      : {total_log_loss:.4f}\")\n",
    "print(f\"  Decision boundary     : ~${decision_boundary:.0f},000/year income\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-wrapup-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What We Built ‚Äî End-to-End Recap\n",
    "\n",
    "Congratulations! You just built a complete machine learning classifier. Here's what happened, step by step:\n",
    "\n",
    "| Step | What We Did | Why It Matters |\n",
    "| :--- | :--- | :--- |\n",
    "| **Data** | Collected historical loan outcomes | The model can only learn from examples |\n",
    "| **Linear Regression Failure** | Showed it gives nonsensical outputs for yes/no problems | Sets up WHY we need something better |\n",
    "| **Sigmoid Function** | Introduced the S-curve that converts any number to a probability (0‚Äì1) | This is what makes Logistic Regression work |\n",
    "| **Classification** | Defined what it means to assign inputs to categories | The fundamental concept behind the whole field |\n",
    "| **Training** | Let `scikit-learn` fit the model to our data | The model learned the best sigmoid for our data |\n",
    "| **Threshold** | Applied the 50% cutoff to turn probability ‚Üí decision | The bridge from \"how likely\" to \"yes or no\" |\n",
    "| **Log-Loss** | Measured how wrong the model's probabilities are | The feedback signal the model uses to improve |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-talking-points",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3 Talking Points for Class Presentation\n",
    "\n",
    "Here are three key ideas you can use to explain this topic to other students ‚Äî each explained in a way that makes it click:\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Talking Point 1: \"Why Logistic, Not Linear?\"\n",
    "\n",
    "> *\"Linear regression draws a straight line to predict a number ‚Äî like predicting tomorrow's temperature. But when the answer is yes or no, a straight line breaks down. It can predict probabilities above 100% or below 0%, which is mathematically impossible. Logistic regression fixes this by using a sigmoid function ‚Äî an S-shaped curve that always stays between 0 and 1, giving us valid probabilities no matter what inputs we feed it. This is the difference between asking 'how much?' (linear) and asking 'which one?' (logistic).\"*\n",
    "\n",
    "**Anchor example:** *\"Imagine asking a straight line: 'What's the probability this unemployed person with $5k in savings will repay a $50k loan?' The line might say -0.4. Logistic regression would say 3% ‚Äî a much more meaningful answer.\"*\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Talking Point 2: \"The Decision Boundary is the Model's Opinion\"\n",
    "\n",
    "> *\"When a logistic regression model is trained, it learns a decision boundary ‚Äî an invisible line in your data that separates 'yes' from 'no'. In our loan example, that boundary is an income level: anyone earning above $X gets approved, below gets rejected. The model's entire job during training is to find the BEST position for that boundary ‚Äî the one that separates the historical defaults from the repayments as cleanly as possible. With more features (income + credit score + debt ratio), the boundary becomes a plane in 3D space, or a hyperplane in higher dimensions ‚Äî but the intuition stays exactly the same.\"*\n",
    "\n",
    "**Anchor example:** *\"Think of the decision boundary as the bank's invisible rule: 'We approve anyone who passes this line.' The machine learns where that line should be ‚Äî instead of a human deciding it with guesswork.\"*\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Talking Point 3: \"Log-Loss Rewards Honesty, Not Just Correctness\"\n",
    "\n",
    "> *\"Most people think you evaluate a model by counting how many predictions were right. But that's incomplete. A model that says 'I'm 51% sure this person will default' and gets it right is less valuable than one that says 'I'm 99% sure' and gets it right. Log-loss captures this: it measures how confident AND correct the model is. A model that says 95% probability and is right gets almost no penalty. A model that says 95% probability and is wrong gets a massive penalty. This forces the model to only be confident when it has good reason to be ‚Äî which is exactly what you want from a financial risk model.\"*\n",
    "\n",
    "**Anchor example:** *\"Imagine a weather forecaster who says '100% chance of sun' and it rains ‚Äî that's catastrophic confidence in the wrong direction. Log-loss would give that forecaster a terrible score. One who said '60% chance of sun' and was wrong gets a much smaller penalty, because they were appropriately uncertain.\"*\n",
    "\n",
    "---\n",
    "\n",
    "## Final Thought\n",
    "\n",
    "Logistic Regression is often described as \"simple\" ‚Äî but it's the foundation of almost everything in machine learning. Neural networks, deep learning, and many modern AI systems are built on the same core principles:\n",
    "\n",
    "1. **Learn a mapping from inputs to probabilities** (the sigmoid is just one way)\n",
    "2. **Apply a threshold to make a decision**\n",
    "3. **Use a loss function to measure and improve**\n",
    "\n",
    "Master these three ideas, and you've understood the DNA of modern machine learning."
   ]
  }
 ]
}
